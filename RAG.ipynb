{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de0459b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(profile={'max_input_tokens': 1047576, 'max_output_tokens': 32768, 'image_inputs': True, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True, 'structured_output': True, 'image_url_inputs': True, 'pdf_inputs': True, 'pdf_tool_message': True, 'image_tool_message': True, 'tool_choice': True}, client=<openai.resources.chat.completions.completions.Completions object at 0x78fe9705dbb0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x78fea04075f0>, root_client=<openai.OpenAI object at 0x78fe9705ca10>, root_async_client=<openai.AsyncOpenAI object at 0x78fe9705e270>, model_name='gpt-4.1', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://yunwu.ai/v1')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 加载.env（如果有的话，没有也不影响）\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# 配置API信息\n",
    "base_url = \"https://yunwu.ai/v1\"\n",
    "openai_api_key = 'sk-mfyuzP5LaqpQ3XT6gKGWpqSyFv75vCG8r4JTAI6gPZff8vGa'\n",
    "\n",
    "# 把api_key和base_url传给ChatOpenAI\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.0,\n",
    "    api_key=openai_api_key,  # 明确传入你的密钥\n",
    "    base_url=base_url,\n",
    "    model=\"gpt-4.1\"        # 明确传入自定义base_url\n",
    ")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "739b4fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm.invoke(\"请你自我介绍一下自己！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9336c1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='你好！我是ChatGPT，一个由OpenAI开发的人工智能语言模型。我的主要功能是理解和生成自然语言文本，可以帮助你解答问题、写作、翻译、学习新知识、编程、头脑风暴等。我可以用中文、英文等多种语言与用户交流。无论你是需要学习辅导、工作协助，还是日常生活中的小帮手，都可以随时向我提问。如果你有任何问题或需要帮助，欢迎随时告诉我！', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 110, 'prompt_tokens': 15, 'total_tokens': 125, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1', 'system_fingerprint': 'fp_b9041e10b4', 'id': 'chatcmpl-CqgxFbkUo8XL0nBLSjPpbcxO2H8US', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b55fd-159d-7ac2-af46-448e75bb9a76-0', usage_metadata={'input_tokens': 15, 'output_tokens': 110, 'total_tokens': 125, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fe00d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里我们要求模型对给定文本进行中文翻译\n",
    "prompt = \"\"\"请你将由三个反引号分割的文本翻译成英文！\\\n",
    "text: ```{text}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c91d6063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'请你将由三个反引号分割的文本翻译成英文！text: ```我带着比身体重的行李，游入尼罗河底，经过几道闪电 看到一堆光圈，不确定是不是这里。```\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "text = \"我带着比身体重的行李，\\\n",
    "游入尼罗河底，\\\n",
    "经过几道闪电 看到一堆光圈，\\\n",
    "不确定是不是这里。\\\n",
    "\"\n",
    "prompt.format(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9badd175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='你是一个翻译助手，可以帮助我将 中文 翻译成 英文.', additional_kwargs={}, response_metadata={}), HumanMessage(content='我带着比身体重的行李，游入尼罗河底，经过几道闪电 看到一堆光圈，不确定是不是这里。', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"你是一个翻译助手，可以帮助我将 {input_language} 翻译成 {output_language}.\"\n",
    "human_template = \"{text}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate([\n",
    "    (\"system\", template),\n",
    "    (\"human\", human_template),\n",
    "])\n",
    "\n",
    "text = \"我带着比身体重的行李，\\\n",
    "游入尼罗河底，\\\n",
    "经过几道闪电 看到一堆光圈，\\\n",
    "不确定是不是这里。\\\n",
    "\"\n",
    "messages  = chat_prompt.invoke({\"input_language\": \"中文\", \"output_language\": \"英文\", \"text\": text})\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69635bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I carried luggage heavier than my own body and swam to the bottom of the Nile. Passing through several flashes of lightning, I saw a cluster of halos, unsure if this was the place.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 63, 'total_tokens': 103, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1', 'system_fingerprint': 'fp_b9041e10b4', 'id': 'chatcmpl-Cqh1c6Y7HblzLU2YZlZMRJTWI1MBY', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b5601-382b-7ae2-857a-1eeedfb576bd-0', usage_metadata={'input_tokens': 63, 'output_tokens': 40, 'total_tokens': 103, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output  = llm.invoke(messages)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a704082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I carried luggage heavier than my own body and swam to the bottom of the Nile. Passing through several flashes of lightning, I saw a cluster of halos, unsure if this was the place.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "output_parser.invoke(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67a5f337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I carried luggage heavier than my own body and swam to the bottom of the Nile. Passing through several flashes of lightning, I saw a cluster of halos, unsure if this was the place.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = chat_prompt | llm | output_parser\n",
    "chain.invoke({\"input_language\":\"中文\", \"output_language\":\"英文\",\"text\": text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dcfbdf",
   "metadata": {},
   "outputs": [
    {
     "ename": "APIAuthenticationError",
     "evalue": "Error code: 401, with error text {\"error\":{\"code\":\"1000\",\"message\":\"身份验证失败。\"}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAPIAuthenticationError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m client = ZhipuAI(api_key=api_key)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# 测试调用（以glm-4模型为例）\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGLM-4.7\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 智谱支持的模型，比如glm-3-turbo、glm-4等\u001b[39;49;00m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m你好，自我介绍一下\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.choices[\u001b[32m0\u001b[39m].message.content)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/zhipuai/api_resource/chat/completions.py:98\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, model, request_id, user_id, do_sample, stream, temperature, top_p, max_tokens, seed, messages, stop, sensitive_word_check, tools, tool_choice, meta, extra, extra_headers, extra_body, timeout, response_format, thinking)\u001b[39m\n\u001b[32m     76\u001b[39m             item[\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m] = drop_prefix_image_data(item[\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     78\u001b[39m body = deepcopy_minimal({\n\u001b[32m     79\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m     80\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrequest_id\u001b[39m\u001b[33m\"\u001b[39m: request_id,\n\u001b[32m   (...)\u001b[39m\u001b[32m     96\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mthinking\u001b[39m\u001b[33m\"\u001b[39m: thinking\n\u001b[32m     97\u001b[39m })\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStreamResponse\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/zhipuai/core/_http_client.py:809\u001b[39m, in \u001b[36mHttpClient.post\u001b[39m\u001b[34m(self, path, cast_type, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m    794\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m    795\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    796\u001b[39m         path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    803\u001b[39m         stream_cls: Type[StreamResponse[Any]] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    804\u001b[39m ) -> ResponseT | StreamResponse:\n\u001b[32m    805\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m    806\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m    807\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m809\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/zhipuai/core/_http_client.py:495\u001b[39m, in \u001b[36mHttpClient.request\u001b[39m\u001b[34m(self, cast_type, options, remaining_retries, stream, stream_cls)\u001b[39m\n\u001b[32m    486\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(\n\u001b[32m    487\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    488\u001b[39m         cast_type: Type[ResponseT],\n\u001b[32m   (...)\u001b[39m\u001b[32m    493\u001b[39m         stream_cls: Type[StreamResponse] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    494\u001b[39m ) -> ResponseT | StreamResponse:\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/zhipuai/core/_http_client.py:582\u001b[39m, in \u001b[36mHttpClient._request\u001b[39m\u001b[34m(self, cast_type, options, remaining_retries, stream, stream_cls)\u001b[39m\n\u001b[32m    579\u001b[39m         err.response.read()\n\u001b[32m    581\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m582\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    584\u001b[39m \u001b[38;5;66;03m# return self._parse_response(\u001b[39;00m\n\u001b[32m    585\u001b[39m \u001b[38;5;66;03m#     cast_type=cast_type,\u001b[39;00m\n\u001b[32m    586\u001b[39m \u001b[38;5;66;03m#     options=options,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    589\u001b[39m \u001b[38;5;66;03m#     stream_cls=stream_cls,\u001b[39;00m\n\u001b[32m    590\u001b[39m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_response(\n\u001b[32m    592\u001b[39m     cast_type=cast_type,\n\u001b[32m    593\u001b[39m     options=options,\n\u001b[32m   (...)\u001b[39m\u001b[32m    596\u001b[39m     stream_cls=stream_cls,\n\u001b[32m    597\u001b[39m )\n",
      "\u001b[31mAPIAuthenticationError\u001b[39m: Error code: 401, with error text {\"error\":{\"code\":\"1000\",\"message\":\"身份验证失败。\"}}"
     ]
    }
   ],
   "source": [
    "from zhipuai import ZhipuaiLLM\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "\n",
    "# 读取本地/项目的环境变量。\n",
    "\n",
    "# find_dotenv()寻找并定位.env文件的路径\n",
    "# load_dotenv()读取该.env文件，并将其中的环境变量加载到当前的运行环境中\n",
    "# 如果你设置的是全局的环境变量，这行代码则没有任何作用。\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# 获取环境变量 API_KEY\n",
    "api_key = \"a743566db8284e20ab3aad041b71d71c.GdCvPQ0ya0LarUxr\"  # 填写控制台中获取的 APIKey 信息\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b36fab14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../C3 搭建知识库\") # 将父目录放入系统路径中\n",
    "\n",
    "# 使用智谱 Embedding API，注意，需要将上一章实现的封装代码下载到本地\n",
    "from zhipuai_embedding import ZhipuAIEmbeddings\n",
    "\n",
    "from langchain.vectorstores.chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b103bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "if not os.getenv(\"ZHIPUAI_API_KEY\"):\n",
    "    os.environ[\"ZHIPUAI_API_KEY\"] = getpass.getpass(\"Enter your ZhipuAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "456917fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 OpenAI Embedding\n",
    "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "# 使用百度千帆 Embedding\n",
    "# from langchain.embeddings.baidu_qianfan_endpoint import QianfanEmbeddingsEndpoint\n",
    "# 使用我们自己封装的智谱 Embedding，需要将封装代码下载到本地使用\n",
    "from zhipuai_embedding import ZhipuAIEmbeddings\n",
    "\n",
    "# 定义 Embeddings\n",
    "# embedding = OpenAIEmbeddings() \n",
    "embedding = ZhipuAIEmbeddings()\n",
    "# embedding = QianfanEmbeddingsEndpoint()\n",
    "\n",
    "# 定义持久化路径\n",
    "#persist_directory = '/workspaces/Xiaoti_Data_Interview_Preparation/knowledge_suanming'\n",
    "#from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# 你的智谱API密钥\n",
    "#zhipuai_api_key = \"a74356d8b8284e20ab3aad041b71d71c.GdCvPQ0ya0LaUxr\"\n",
    "\n",
    "# 初始化Embedding（现在能传api_key了）\n",
    "#embedding = ZhipuAIEmbeddings(api_key=zhipuai_api_key)\n",
    "\n",
    "# 加载向量库\n",
    "persist_directory = '/workspaces/Xiaoti_Data_Interview_Preparation/knowledge_suanming'\n",
    "vectordb = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "915df183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检索到的内容数：3\n"
     ]
    }
   ],
   "source": [
    "question = \"我的生日是2004年1月16日，请问我的命运是怎么样的？\"\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})\n",
    "docs = retriever.invoke(question)\n",
    "print(f\"检索到的内容数：{len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "98e1b696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检索到的第0个内容: \n",
      " 新十二星座分析\n",
      "-----------------------------------------------------\n",
      "检索到的第1个内容: \n",
      " 先天后天论\n",
      "-----------------------------------------------------\n",
      "检索到的第2个内容: \n",
      " 以上八型，如果能融汇贯通，随心所欲，青出于蓝而胜于蓝者，真高人也\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(docs):\n",
    "    print(f\"检索到的第{i}个内容: \\n {doc.page_content}\", end=\"\\n-----------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "934d1c1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'新十二星座分析\\n\\n先天后天论\\n\\n以上八型，如果能融汇贯通，随心所欲，青出于蓝而胜于蓝者，真高人也'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "def combine_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "combiner = RunnableLambda(combine_docs)\n",
    "retrieval_chain = retriever | combiner\n",
    "\n",
    "retrieval_chain.invoke(\"我的生日是2004年1月16日，请问我的命运是怎么样的？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00759f1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
